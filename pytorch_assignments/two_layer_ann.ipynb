{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Custom ANN Implementation (2-4-1 Architecture)\n",
    "\n",
    "**Assignment: Add One Hidden Layer**\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input Layer: 2 neurons\n",
    "- Hidden Layer: 4 neurons (ReLU activation)\n",
    "- Output Layer: 1 neuron (Sigmoid activation)\n",
    "- Loss: Binary Cross Entropy\n",
    "- Optimizer: Manual weight update using gradients with .backward()\n",
    "\n",
    "**Network Structure: 2-4-1**\n",
    "\n",
    "**Forward Pass:**\n",
    "```\n",
    "Z1 = X @ W1 + b1\n",
    "A1 = torch.relu(Z1)\n",
    "Z2 = A1 @ W2 + b2\n",
    "Y_pred = torch.sigmoid(Z2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset (Same as Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV (same as Q2)\n",
    "def load_dataset(csv_path='binary_data.csv'):\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Loading dataset from: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        X = df[['f1', 'f2']].values\n",
    "        y = df['label'].values\n",
    "        print(f\"Loaded dataset shape: {df.shape}\")\n",
    "        print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "        return X, y, df\n",
    "    else:\n",
    "        print(f\"CSV file {csv_path} not found. Please run Q2 first to generate the dataset.\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load the dataset\n",
    "X, y, df = load_dataset()\n",
    "\n",
    "if X is not None:\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Visualize the dataset\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    plt.xlabel('Feature 1 (f1)')\n",
    "    plt.ylabel('Feature 2 (f2)')\n",
    "    plt.title('Binary Classification Dataset (Same as Q2)')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\nelse:\n",
    "    print(\"Please run the single_layer_ann.py script first to generate the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training (same as Q2)\n",
    "def prepare_data(X, y, test_size=0.2, random_state=42):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1).to(device)\n",
    "    \n",
    "    print(f\"Training set: {X_train_tensor.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test_tensor.shape[0]} samples\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, device\n",
    "\n",
    "if X is not None:\n",
    "    # Prepare the data\n",
    "    X_train, X_test, y_train, y_test, device = prepare_data(X, y)\n",
    "    \n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    print(f\"y_train: {y_train.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Network Parameters (2-4-1 Architecture)\n",
    "\n",
    "As specified in the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters as specified in the assignment\n",
    "if X is not None:\n",
    "    print(\"Initializing 2-4-1 Neural Network parameters...\")\n",
    "    \n",
    "    # Layer 1: Input (2) -> Hidden (4)\n",
    "    W1 = torch.randn(2, 4, requires_grad=True, device=device)\n",
    "    b1 = torch.zeros(1, 4, requires_grad=True, device=device)\n",
    "    \n",
    "    # Layer 2: Hidden (4) -> Output (1)\n",
    "    W2 = torch.randn(4, 1, requires_grad=True, device=device)\n",
    "    b2 = torch.zeros(1, 1, requires_grad=True, device=device)\n",
    "    \n",
    "    print(f\"\\nNetwork Architecture: 2-4-1\")\n",
    "    print(f\"W1 shape: {W1.shape} (Input -> Hidden)\")\n",
    "    print(f\"b1 shape: {b1.shape}\")\n",
    "    print(f\"W2 shape: {W2.shape} (Hidden -> Output)\")\n",
    "    print(f\"b2 shape: {b2.shape}\")\n",
    "    print(f\"Total parameters: {2*4 + 4 + 4*1 + 1} = 17\")\n",
    "    \n",
    "    # Display initial weights\n",
    "    print(f\"\\nInitial weights:\")\n",
    "    print(f\"W1:\\n{W1.detach().cpu().numpy()}\")\n",
    "    print(f\"b1: {b1.detach().cpu().numpy()}\")\n",
    "    print(f\"W2:\\n{W2.detach().cpu().numpy()}\")\n",
    "    print(f\"b2: {b2.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass Implementation\n",
    "\n",
    "Implement the forward pass as specified:\n",
    "```\n",
    "Z1 = X @ W1 + b1\n",
    "A1 = torch.relu(Z1)\n",
    "Z2 = A1 @ W2 + b2\n",
    "Y_pred = torch.sigmoid(Z2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass through the 2-4-1 network\n",
    "    \n",
    "    Forward Pass:\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = torch.relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    Y_pred = torch.sigmoid(Z2)\n",
    "    \"\"\"\n",
    "    # Layer 1: Linear transformation + ReLU activation\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = torch.relu(Z1)\n",
    "    \n",
    "    # Layer 2: Linear transformation + Sigmoid activation\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    Y_pred = torch.sigmoid(Z2)\n",
    "    \n",
    "    return Y_pred, A1, Z1  # Return intermediate values for analysis\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred, y_true):\n",
    "    \"\"\"Binary Cross Entropy Loss\"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    return loss.mean()\n",
    "\n",
    "if X is not None:\n",
    "    # Test the forward pass\n",
    "    print(\"Testing forward pass...\")\n",
    "    with torch.no_grad():\n",
    "        y_pred, A1, Z1 = forward_pass(X_train[:5], W1, b1, W2, b2)\n",
    "        print(f\"Sample predictions (first 5): {y_pred.detach().cpu().numpy().flatten()}\")\n",
    "        print(f\"Hidden layer activations shape: {A1.shape}\")\n",
    "        print(f\"Sample hidden activations (first sample): {A1[0].detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Manual Weight Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, y, W1, b1, W2, b2, learning_rate):\n",
    "    \"\"\"\n",
    "    Single training step using automatic differentiation and manual weight updates\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    y_pred, _, _ = forward_pass(X, W1, b1, W2, b2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy_loss(y_pred, y)\n",
    "    \n",
    "    # Backward pass using automatic differentiation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manual weight update using gradients\n",
    "    with torch.no_grad():\n",
    "        # Update weights and biases\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        \n",
    "        # Zero gradients after update\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def calculate_accuracy(X, y, W1, b1, W2, b2):\n",
    "    \"\"\"Calculate accuracy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred, _, _ = forward_pass(X, W1, b1, W2, b2)\n",
    "        y_pred_binary = (y_pred >= 0.5).float()\n",
    "        correct = (y_pred_binary == y).float().sum()\n",
    "        accuracy = (correct / y.shape[0]) * 100\n",
    "    return accuracy.item()\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Training parameters\n",
    "    epochs = 50\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # Storage for metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    print(f\"Training Two-Layer ANN (2-4-1) for {epochs} epochs...\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training step\n",
    "        loss = train_step(X_train, y_train, W1, b1, W2, b2, learning_rate)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_acc = calculate_accuracy(X_train, y_train, W1, b1, W2, b2)\n",
    "        test_acc = calculate_accuracy(X_test, y_test, W1, b1, W2, b2)\n",
    "        \n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "            print(f\"Epoch {epoch:2d}: Loss = {loss:.4f}, Train Acc = {train_acc:.1f}%, Test Acc = {test_acc:.1f}%\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Final results\n",
    "    final_train_acc = train_accuracies[-1]\n",
    "    final_test_acc = test_accuracies[-1]\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Training Accuracy: {final_train_acc:.1f}%\")\n",
    "    print(f\"Test Accuracy: {final_test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results in Assignment Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Display results in the assignment's sample output format\n",
    "    print(\"=\" * 50)\n",
    "    print(\"SAMPLE OUTPUT FORMAT (as requested):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Epoch 1: Loss = {train_losses[0]:.2f}\")\n",
    "    if len(train_losses) >= 30:\n",
    "        print(f\"Epoch 30: Loss = {train_losses[29]:.2f}\")\n",
    "    print(f\"Accuracy: {test_accuracies[-1]:.1f}%\")\n",
    "    \n",
    "    # Show final model parameters\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINAL MODEL PARAMETERS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"W1 (Input -> Hidden):\\n{W1.detach().cpu().numpy()}\")\n",
    "    print(f\"b1: {b1.detach().cpu().numpy()}\")\n",
    "    print(f\"W2 (Hidden -> Output):\\n{W2.detach().cpu().numpy()}\")\n",
    "    print(f\"b2: {b2.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Create comprehensive visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Original dataset\n",
    "    scatter = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    ax1.set_xlabel('Feature 1 (f1)')\n",
    "    ax1.set_ylabel('Feature 2 (f2)')\n",
    "    ax1.set_title('Original Dataset')\n",
    "    plt.colorbar(scatter, ax=ax1)\n",
    "    \n",
    "    # Plot 2: Training loss\n",
    "    ax2.plot(range(1, len(train_losses) + 1), train_losses, 'b-', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Loss (2-4-1 Network)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy comparison\n",
    "    ax3.plot(range(1, len(train_accuracies) + 1), train_accuracies, 'g-', label='Train Accuracy', linewidth=2)\n",
    "    ax3.plot(range(1, len(test_accuracies) + 1), test_accuracies, 'r-', label='Test Accuracy', linewidth=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.set_title('Training vs Test Accuracy')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Hidden layer activations for a sample\n",
    "    with torch.no_grad():\n",
    "        _, A1_sample, _ = forward_pass(X_train[:10], W1, b1, W2, b2)\n",
    "        A1_np = A1_sample.detach().cpu().numpy()\n",
    "    \n",
    "    im = ax4.imshow(A1_np.T, cmap='viridis', aspect='auto')\n",
    "    ax4.set_xlabel('Sample Index')\n",
    "    ax4.set_ylabel('Hidden Neuron')\n",
    "    ax4.set_title('Hidden Layer Activations (First 10 samples)')\n",
    "    plt.colorbar(im, ax=ax4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Training visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Architecture Comparison and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ASSIGNMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nTwo-Layer Network Architecture (2-4-1):\")\n",
    "    print(f\"  Input Layer: 2 neurons\")\n",
    "    print(f\"  Hidden Layer: 4 neurons (ReLU activation)\")\n",
    "    print(f\"  Output Layer: 1 neuron (Sigmoid activation)\")\n",
    "    print(f\"  Total parameters: {2*4 + 4 + 4*1 + 1} = 17\")\n",
    "    \n",
    "    print(\"\\nTraining Summary:\")\n",
    "    print(f\"  Total epochs: {epochs}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Loss reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n",
    "    \n",
    "    print(\"\\nFinal Performance:\")\n",
    "    print(f\"  Training accuracy: {final_train_acc:.1f}%\")\n",
    "    print(f\"  Test accuracy: {final_test_acc:.1f}%\")\n",
    "    \n",
    "    print(\"\\n✓ All requirements fulfilled:\")\n",
    "    print(\"  ✓ Used same dataset as Q2\")\n",
    "    print(\"  ✓ Implemented 2-4-1 architecture as specified\")\n",
    "    print(\"  ✓ Used specified initialization: W1=randn(2,4), b1=zeros(1,4), etc.\")\n",
    "    print(\"  ✓ Implemented forward pass: Z1=X@W1+b1, A1=relu(Z1), Z2=A1@W2+b2, Y=sigmoid(Z2)\")\n",
    "    print(\"  ✓ Used Binary Cross Entropy loss\")\n",
    "    print(\"  ✓ Used .backward() for automatic differentiation\")\n",
    "    print(\"  ✓ Manual weight updates with torch.no_grad()\")\n",
    "    print(\"  ✓ Proper gradient zeroing with .grad.zero_()\")\n",
    "    print(\"  ✓ Tracked loss and accuracy as requested\")\nelse:\n",
    "    print(\"Please run the single_layer_ann.py script first to generate the dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
